{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "torch.__version__='2.0.0+cu117'\n",
      "torch.__file__='/home/ubuntu/anaconda3/envs/bedict_crispr/lib/python3.10/site-packages/torch/__init__.py'\n",
      "torch.cuda.device_count()=1\n",
      "torch.cuda.is_available()=True\n",
      "torch.version.cuda='11.7'\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.executable\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import ogb\n",
    "from tqdm import tqdm\n",
    "import hiplot as hip\n",
    "from copy import deepcopy\n",
    "import datetime\n",
    "import json\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import Subset, TensorDataset\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "print(f\"{torch.__version__=}\")\n",
    "print(f\"{torch.__file__=}\")\n",
    "print(f\"{torch.cuda.device_count()=}\")\n",
    "print(f\"{torch.cuda.is_available()=}\")\n",
    "print(f\"{torch.version.cuda=}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mnt/graphnn/notebooks\n",
      "/home/ubuntu/mnt/graphnn\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "cwd_parent = os.path.abspath(os.path.join(cwd, os.pardir))\n",
    "print(cwd_parent)\n",
    "\n",
    "sys.path.append(cwd_parent)\n",
    "\n",
    "import deepadr\n",
    "from deepadr.dataset import *\n",
    "from deepadr.utilities import *\n",
    "from deepadr.chemfeatures import *\n",
    "from deepadr.train_functions import *\n",
    "from deepadr.model_gnn_ogb import GNN, DeepAdr_SiameseTrf, ExpressionNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version\n",
      "3.10.9 (main, Jan 11 2023, 15:21:40) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python version\")\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_dir = '../data/raw/'\n",
    "processed_dir = '../data/processed/'\n",
    "up_dir = '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of GPUs available: 1\n",
      "cuda:0, name:Tesla T4\n",
      "total memory available: 14.755615234375 GB\n",
      "total memory allocated on device: 0.0 GB\n",
      "max memory allocated on device: 0.0 GB\n",
      "total memory cached on device: 0.0 GB\n",
      "max memory cached  on device: 0.0 GB\n",
      "\n",
      "torch: 2.0.0+cu117\n",
      "CUDA: 11.7\n",
      "3.10.9 (main, Jan 11 2023, 15:21:40) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "report_available_cuda_devices()\n",
    "n_gpu = torch.cuda.device_count()\n",
    "n_gpu\n",
    "device_cpu = get_device(to_gpu=True)\n",
    "# device_gpu = get_device(True, index=0)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"CUDA:\", torch.version.cuda)\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options: \n",
    "# 'total_thresh' + 4,3,2\n",
    "# 'loewe_thresh', 'hsa_thresh', 'bliss_thresh', 'zip_thresh' + 1\n",
    "\n",
    "score = 'bliss_thresh'\n",
    "score_val = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DrugComb_bliss_thresh_1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DSdataset_name = f'DrugComb_{score}_{score_val}'\n",
    "data_fname = 'data_v1' # v2 for baseline models, v3 for additive samples\n",
    "DSdataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mnt/graphnn/data/processed/DrugComb_bliss_thresh_1/data_v1\n"
     ]
    }
   ],
   "source": [
    "targetdata_dir = create_directory(os.path.join(processed_dir, DSdataset_name, data_fname))\n",
    "targetdata_dir_raw = create_directory(os.path.join(targetdata_dir, \"raw\"))\n",
    "targetdata_dir_processed = create_directory(os.path.join(targetdata_dir, \"processed\"))\n",
    "targetdata_dir_exp = create_directory(os.path.join(targetdata_dir, \"experiments\"))\n",
    "# # ReaderWriter.dump_data(dpartitions, os.path.join(targetdata_dir, 'data_partitions.pkl'))\n",
    "print(targetdata_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 391 ms, total: 391 ms\n",
      "Wall time: 508 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Make sure to first run the \"DDoS_Dataset_Generation\" notebook first\n",
    "dataset = MoleculeDataset(root=targetdata_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(targetdata_dir_raw +'/data_pairs.pkl', 'rb') as file:\n",
    "    data_pairs = pickle.load(file )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drug1_ID</th>\n",
       "      <th>Drug2_ID</th>\n",
       "      <th>Cell_Line_ID</th>\n",
       "      <th>Cosmic_ID</th>\n",
       "      <th>Drug1</th>\n",
       "      <th>Drug2</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ethyl bromopyruvate</td>\n",
       "      <td>717906-29-1</td>\n",
       "      <td>A-673</td>\n",
       "      <td>684052</td>\n",
       "      <td>CCOC(=O)C(=O)CBr</td>\n",
       "      <td>CN(C1=CC=CC=C1CNC2=NC(=NC=C2C(F)(F)F)NC3=CC4=C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dacarbazine</td>\n",
       "      <td>717906-29-1</td>\n",
       "      <td>A-673</td>\n",
       "      <td>684052</td>\n",
       "      <td>CN(C)N=NC1=C(NC=N1)C(=O)N</td>\n",
       "      <td>CN(C1=CC=CC=C1CNC2=NC(=NC=C2C(F)(F)F)NC3=CC4=C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chlorambucil</td>\n",
       "      <td>717906-29-1</td>\n",
       "      <td>A-673</td>\n",
       "      <td>684052</td>\n",
       "      <td>C1=CC(=CC=C1CCCC(=O)O)N(CCCl)CCCl</td>\n",
       "      <td>CN(C1=CC=CC=C1CNC2=NC(=NC=C2C(F)(F)F)NC3=CC4=C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nedaplatin</td>\n",
       "      <td>717906-29-1</td>\n",
       "      <td>A-673</td>\n",
       "      <td>684052</td>\n",
       "      <td>C(C(=O)O)O.N.N.[Pt]</td>\n",
       "      <td>CN(C1=CC=CC=C1CNC2=NC(=NC=C2C(F)(F)F)NC3=CC4=C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>colchicine</td>\n",
       "      <td>717906-29-1</td>\n",
       "      <td>A-673</td>\n",
       "      <td>684052</td>\n",
       "      <td>CC(=O)NC1CCC2=CC(=C(C(=C2C3=CC=C(C(=O)C=C13)OC...</td>\n",
       "      <td>CN(C1=CC=CC=C1CNC2=NC(=NC=C2C(F)(F)F)NC3=CC4=C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125543</th>\n",
       "      <td>957054-30-7</td>\n",
       "      <td>Selumetinib</td>\n",
       "      <td>VM-CUB-1</td>\n",
       "      <td>909780</td>\n",
       "      <td>CS(=O)(=O)N1CCN(CC1)CC2=CC3=C(S2)C(=NC(=N3)C4=...</td>\n",
       "      <td>CN1C=NC2=C1C=C(C(=C2F)NC3=C(C=C(C=C3)Br)Cl)C(=...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125544</th>\n",
       "      <td>AZD1208</td>\n",
       "      <td>Vorinostat</td>\n",
       "      <td>VM-CUB-1</td>\n",
       "      <td>909780</td>\n",
       "      <td>C1CC(CN(C1)C2=C(C=CC=C2C=C3C(=O)NC(=O)S3)C4=CC...</td>\n",
       "      <td>C1=CC=C(C=C1)NC(=O)CCCCCCC(=O)NO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125545</th>\n",
       "      <td>Saracatinib</td>\n",
       "      <td>AZD4320</td>\n",
       "      <td>VM-CUB-1</td>\n",
       "      <td>909780</td>\n",
       "      <td>CN1CCN(CC1)CCOC2=CC(=C3C(=C2)N=CN=C3NC4=C(C=CC...</td>\n",
       "      <td>CN(CCC(CSC1=CC=CC=C1)NC2=C(C=C(C=C2)S(=O)(=O)N...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125546</th>\n",
       "      <td>Saracatinib</td>\n",
       "      <td>AZD5582</td>\n",
       "      <td>VM-CUB-1</td>\n",
       "      <td>909780</td>\n",
       "      <td>CN1CCN(CC1)CCOC2=CC(=C3C(=C2)N=CN=C3NC4=C(C=CC...</td>\n",
       "      <td>CC(C(=O)NC(C1CCCCC1)C(=O)N2CCCC2C(=O)NC3C(CC4=...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125547</th>\n",
       "      <td>Saracatinib</td>\n",
       "      <td>Olaparib</td>\n",
       "      <td>VM-CUB-1</td>\n",
       "      <td>909780</td>\n",
       "      <td>CN1CCN(CC1)CCOC2=CC(=C3C(=C2)N=CN=C3NC4=C(C=CC...</td>\n",
       "      <td>C1CC1C(=O)N2CCN(CC2)C(=O)C3=C(C=CC(=C3)CC4=NNC...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125548 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Drug1_ID     Drug2_ID Cell_Line_ID  Cosmic_ID   \n",
       "0       Ethyl bromopyruvate  717906-29-1        A-673     684052  \\\n",
       "1               dacarbazine  717906-29-1        A-673     684052   \n",
       "2              chlorambucil  717906-29-1        A-673     684052   \n",
       "3                Nedaplatin  717906-29-1        A-673     684052   \n",
       "4                colchicine  717906-29-1        A-673     684052   \n",
       "...                     ...          ...          ...        ...   \n",
       "125543          957054-30-7  Selumetinib     VM-CUB-1     909780   \n",
       "125544              AZD1208   Vorinostat     VM-CUB-1     909780   \n",
       "125545          Saracatinib      AZD4320     VM-CUB-1     909780   \n",
       "125546          Saracatinib      AZD5582     VM-CUB-1     909780   \n",
       "125547          Saracatinib     Olaparib     VM-CUB-1     909780   \n",
       "\n",
       "                                                    Drug1   \n",
       "0                                        CCOC(=O)C(=O)CBr  \\\n",
       "1                               CN(C)N=NC1=C(NC=N1)C(=O)N   \n",
       "2                       C1=CC(=CC=C1CCCC(=O)O)N(CCCl)CCCl   \n",
       "3                                     C(C(=O)O)O.N.N.[Pt]   \n",
       "4       CC(=O)NC1CCC2=CC(=C(C(=C2C3=CC=C(C(=O)C=C13)OC...   \n",
       "...                                                   ...   \n",
       "125543  CS(=O)(=O)N1CCN(CC1)CC2=CC3=C(S2)C(=NC(=N3)C4=...   \n",
       "125544  C1CC(CN(C1)C2=C(C=CC=C2C=C3C(=O)NC(=O)S3)C4=CC...   \n",
       "125545  CN1CCN(CC1)CCOC2=CC(=C3C(=C2)N=CN=C3NC4=C(C=CC...   \n",
       "125546  CN1CCN(CC1)CCOC2=CC(=C3C(=C2)N=CN=C3NC4=C(C=CC...   \n",
       "125547  CN1CCN(CC1)CCOC2=CC(=C3C(=C2)N=CN=C3NC4=C(C=CC...   \n",
       "\n",
       "                                                    Drug2  Y  \n",
       "0       CN(C1=CC=CC=C1CNC2=NC(=NC=C2C(F)(F)F)NC3=CC4=C...  0  \n",
       "1       CN(C1=CC=CC=C1CNC2=NC(=NC=C2C(F)(F)F)NC3=CC4=C...  0  \n",
       "2       CN(C1=CC=CC=C1CNC2=NC(=NC=C2C(F)(F)F)NC3=CC4=C...  0  \n",
       "3       CN(C1=CC=CC=C1CNC2=NC(=NC=C2C(F)(F)F)NC3=CC4=C...  0  \n",
       "4       CN(C1=CC=CC=C1CNC2=NC(=NC=C2C(F)(F)F)NC3=CC4=C...  0  \n",
       "...                                                   ... ..  \n",
       "125543  CN1C=NC2=C1C=C(C(=C2F)NC3=C(C=C(C=C3)Br)Cl)C(=...  1  \n",
       "125544                   C1=CC=C(C=C1)NC(=O)CCCCCCC(=O)NO  0  \n",
       "125545  CN(CCC(CSC1=CC=CC=C1)NC2=C(C=C(C=C2)S(=O)(=O)N...  0  \n",
       "125546  CC(C(=O)NC(C1CCCCC1)C(=O)N2CCCC2C(=O)NC3C(CC4=...  1  \n",
       "125547  C1CC1C(=O)N2CCN(CC2)C(=O)C3=C(C=CC(=C3)CC4=NNC...  0  \n",
       "\n",
       "[125548 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This file is to leav a cell out partition\n",
    "def data_partition_leave_cells_out(num_folds,random_state,valid_set_portion,path_to_save, feature ):\n",
    "    import pickle\n",
    "    with open(targetdata_dir_raw +'/data_pairs.pkl', 'rb') as file:\n",
    "        data_pairs = pickle.load(file )\n",
    "        \n",
    "    #grouped = data_pairs.groupby('Cosmic_ID')['Y'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "    grouped = data_pairs.groupby(feature)['Y'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "    ### we lable those have mostly 0 as class0, blanced 0 and 1 as class 1 and similar distribution to \n",
    "    ## the original class distribution, around 0.81 as 2 and more balanced ones are 3\n",
    "    label = []\n",
    "    for i in range(len(grouped)):\n",
    "        if grouped[0].iloc[i]>=0.90:\n",
    "            label.append(0)\n",
    "        elif grouped[0].iloc[i]<0.2:\n",
    "            label.append(1)\n",
    "        elif grouped[0].iloc[i]<0.90 and grouped[0].iloc[i]>0.75:\n",
    "            label.append(2)\n",
    "        else:\n",
    "            label.append(3)   \n",
    "            \n",
    "    grouped['label'] = label\n",
    "    grouped = grouped.reset_index()\n",
    "    \n",
    "    X = grouped[feature]\n",
    "    Y = grouped['label']\n",
    "    \n",
    "    skf_trte = StratifiedKFold(n_splits=num_folds, random_state=random_state, shuffle=True)  # split train and test\n",
    "    skf_trv = StratifiedShuffleSplit(n_splits=2, \n",
    "                                     test_size=valid_set_portion, \n",
    "                                     random_state=random_state)  # split train and validation\n",
    "\n",
    "    data_partition = {}\n",
    "    fold_num = 0\n",
    "    \n",
    "    for train_index, test_index in skf_trte.split(X,Y):\n",
    "        x_tr = np.zeros(len(train_index))\n",
    "        y_tr = Y[train_index] \n",
    "        #print('train cell line number', len(train_index))\n",
    "        print('test', feature,  'number', len(test_index))\n",
    "\n",
    "        for tr_index, val_index in skf_trv.split(x_tr, y_tr):\n",
    "            tr_ids = train_index[tr_index]\n",
    "            val_ids = train_index[val_index]\n",
    "            print('train', feature ,' number', len(tr_ids))\n",
    "            print('validation', feature,  'number' , len(val_ids))\n",
    "            \n",
    "            data_partition[fold_num] = {'train': tr_ids,\n",
    "                                             'validation': val_ids,\n",
    "                                             'test': test_index}\n",
    "            \n",
    "            \n",
    "            \n",
    "        fold_num = fold_num + 1\n",
    "    \n",
    "    \n",
    "    data_partitions = {}\n",
    "   \n",
    "    YY = data_pairs.Y\n",
    "    for i in range(num_folds):\n",
    "        train_cell = X[data_partition[i]['train']]\n",
    "        val_cell = X[data_partition[i]['validation']]\n",
    "        test_cell = X[data_partition[i]['test']]\n",
    "        train_indices = np.array(data_pairs.loc[data_pairs[feature].isin(train_cell)].index)\n",
    "        val_indices = np.array(data_pairs.loc[data_pairs[feature].isin(val_cell)].index)\n",
    "        test_indices = np.array(data_pairs.loc[data_pairs[feature].isin(test_cell)].index)\n",
    "        data_partitions[i] = {'train': train_indices,\n",
    "                                             'validation': val_indices,\n",
    "                                             'test': test_indices}\n",
    "        \n",
    "    \n",
    "    \n",
    "        print(\"fold_num:\", i)\n",
    "        print('train data')\n",
    "        report_label_distrib(YY[data_partitions[i]['train']])\n",
    "        print('validation data')\n",
    "        report_label_distrib(YY[data_partitions[i]['validation']])\n",
    "        print('test data')\n",
    "        report_label_distrib(YY[data_partitions[i]['test']])\n",
    "        print()\n",
    "        fold_num += 1\n",
    "        print(\"-\"*25)\n",
    "    \n",
    "    file_name = path_to_save + f'/partions_{feature}_out.pkl'\n",
    "    with open(file_name, \"wb\") as pickle_file:\n",
    "        pickle.dump(data_partitions, pickle_file)\n",
    "    return data_partitions\n",
    "\n",
    "\n",
    "def report_label_distrib(labels):\n",
    "    classes, counts = np.unique(labels, return_counts=True)\n",
    "    norm_counts = counts/counts.sum()\n",
    "    for i, label in enumerate(classes):\n",
    "        print(\"class:\", label, \"norm count:\", norm_counts[i])\n",
    "\n",
    "\n",
    "def validate_partitioan(train_indices, val_indices, test_indices):\n",
    "    # Find the intersection between test_indices and val_indices\n",
    "    test_val_intersection = set(test_indices).intersection(val_indices)\n",
    "\n",
    "    # Find the intersection between test_indices and train_indices\n",
    "    test_train_intersection = set(test_indices).intersection(train_indices)\n",
    "\n",
    "    # Find the intersection between val_indices and train_indices\n",
    "    val_train_intersection = set(val_indices).intersection(train_indices)\n",
    "\n",
    "    # Check if any of the intersections have common elements\n",
    "    if test_val_intersection or test_train_intersection or val_train_intersection:\n",
    "        print(\"The sets of indices intersect.\")\n",
    "    else:\n",
    "        print(\"The sets of indices do not intersect.\")\n",
    "\n",
    "    # Calculate the total count of indices\n",
    "    total_indices_count = len(test_indices) + len(val_indices) + len(train_indices)\n",
    "\n",
    "    # Print the total count of indices\n",
    "    print(\"Total count of indices:\", total_indices_count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Drug1_ID number 368\n",
      "train Drug1_ID  number 1322\n",
      "validation Drug1_ID number 147\n",
      "train Drug1_ID  number 1322\n",
      "validation Drug1_ID number 147\n",
      "test Drug1_ID number 368\n",
      "train Drug1_ID  number 1322\n",
      "validation Drug1_ID number 147\n",
      "train Drug1_ID  number 1322\n",
      "validation Drug1_ID number 147\n",
      "test Drug1_ID number 367\n",
      "train Drug1_ID  number 1323\n",
      "validation Drug1_ID number 147\n",
      "train Drug1_ID  number 1323\n",
      "validation Drug1_ID number 147\n",
      "test Drug1_ID number 367\n",
      "train Drug1_ID  number 1323\n",
      "validation Drug1_ID number 147\n",
      "train Drug1_ID  number 1323\n",
      "validation Drug1_ID number 147\n",
      "test Drug1_ID number 367\n",
      "train Drug1_ID  number 1323\n",
      "validation Drug1_ID number 147\n",
      "train Drug1_ID  number 1323\n",
      "validation Drug1_ID number 147\n",
      "fold_num: 0\n",
      "train data\n",
      "class: 0 norm count: 0.5003300931090218\n",
      "class: 1 norm count: 0.49966990689097823\n",
      "validation data\n",
      "class: 0 norm count: 0.5151309335144517\n",
      "class: 1 norm count: 0.4848690664855483\n",
      "test data\n",
      "class: 0 norm count: 0.518389723029088\n",
      "class: 1 norm count: 0.4816102769709119\n",
      "\n",
      "-------------------------\n",
      "fold_num: 1\n",
      "train data\n",
      "class: 0 norm count: 0.4975124378109453\n",
      "class: 1 norm count: 0.5024875621890548\n",
      "validation data\n",
      "class: 0 norm count: 0.5701716271996524\n",
      "class: 1 norm count: 0.4298283728003476\n",
      "test data\n",
      "class: 0 norm count: 0.5095894516032364\n",
      "class: 1 norm count: 0.49041054839676357\n",
      "\n",
      "-------------------------\n",
      "fold_num: 2\n",
      "train data\n",
      "class: 0 norm count: 0.49656747971175924\n",
      "class: 1 norm count: 0.5034325202882408\n",
      "validation data\n",
      "class: 0 norm count: 0.508727420336919\n",
      "class: 1 norm count: 0.49127257966308097\n",
      "test data\n",
      "class: 0 norm count: 0.5322964780600462\n",
      "class: 1 norm count: 0.4677035219399538\n",
      "\n",
      "-------------------------\n",
      "fold_num: 3\n",
      "train data\n",
      "class: 0 norm count: 0.5201204364102623\n",
      "class: 1 norm count: 0.4798795635897377\n",
      "validation data\n",
      "class: 0 norm count: 0.43777264562483964\n",
      "class: 1 norm count: 0.5622273543751604\n",
      "test data\n",
      "class: 0 norm count: 0.4867587143503848\n",
      "class: 1 norm count: 0.5132412856496152\n",
      "\n",
      "-------------------------\n",
      "fold_num: 4\n",
      "train data\n",
      "class: 0 norm count: 0.5123764169988094\n",
      "class: 1 norm count: 0.4876235830011905\n",
      "validation data\n",
      "class: 0 norm count: 0.5061615700593336\n",
      "class: 1 norm count: 0.49383842994066635\n",
      "test data\n",
      "class: 0 norm count: 0.47174203774332557\n",
      "class: 1 norm count: 0.5282579622566744\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "#data_partitions =data_partition_leave_cells_out(5, 42, 0.1, targetdata_dir_processed,'Drug1_ID' )\n",
    "partition_style = 'Drug1_ID'  #'Drug1_ID'  #'Cell_Line_ID'\n",
    "data_partitions =data_partition_leave_cells_out(5, 42, 0.1, targetdata_dir_processed,partition_style )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used_dataset = dataset\n",
    "# If you want to use a smaller subset of the dataset for testing\n",
    "#smaller_dataset_len = int(len(dataset)/100)\n",
    "#used_dataset = dataset[:smaller_dataset_len]\n",
    "\n",
    "#dataset = used_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfold_partitions = get_stratified_partitions(dataset.data.y,\\n                                            num_folds=5,\\n                                            valid_set_portion=0.1,\\n                                            random_state=42)\\n                                            \\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "fold_partitions = get_stratified_partitions(dataset.data.y,\n",
    "                                            num_folds=5,\n",
    "                                            valid_set_portion=0.1,\n",
    "                                            random_state=42)\n",
    "                                            \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_partitions = data_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 81057\n",
      "Number of validation graphs: 2551\n",
      "Number of testing graphs: 24951\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training graphs: \"+ str(len(fold_partitions[0]['train'])))\n",
    "print(\"Number of validation graphs: \"+ str(len(fold_partitions[0]['validation'])))\n",
    "print(\"Number of testing graphs: \"+ str(len(fold_partitions[0]['test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "\n",
    "tp = {\n",
    "    \"batch_size\" : 300,\n",
    "    \"num_epochs\" : 100,\n",
    "    \n",
    "    \"emb_dim\" : 100,\n",
    "    \"gnn_type\" : \"gatv2\",\n",
    "    \"num_layer\" : 5,\n",
    "    \"graph_pooling\" : \"mean\", #attention\n",
    "    \n",
    "    \"input_embed_dim\" : None,\n",
    "    \"gene_embed_dim\": 1,\n",
    "    \"num_attn_heads\" : 2,\n",
    "    \"num_transformer_units\" : 1,\n",
    "    \"p_dropout\" : 0.3,\n",
    "    \"nonlin_func\" : 'nn.ReLU()',\n",
    "    \"mlp_embed_factor\" : 2,\n",
    "    \"pooling_mode\" : 'attn',\n",
    "    \"dist_opt\" : 'cosine',\n",
    "\n",
    "    \"base_lr\" : 3e-4, #3e-4\n",
    "    \"max_lr_mul\": 10,\n",
    "    \"l2_reg\" : 1e-3,  #\"l2_reg\" : 1e-7,\n",
    "    \"loss_w\" : 1.,\n",
    "    \"margin_v\" : 1.,\n",
    "\n",
    "    \"expression_dim\" : 64,\n",
    "    \"expression_input_size\" : 908,\n",
    "    \"exp_H1\" : 4096,\n",
    "    \"exp_H2\" : 1024\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu: cuda:0\n",
      "{'batch_size': 300, 'num_epochs': 100, 'emb_dim': 100, 'gnn_type': 'gatv2', 'num_layer': 5, 'graph_pooling': 'mean', 'input_embed_dim': None, 'gene_embed_dim': 1, 'num_attn_heads': 2, 'num_transformer_units': 1, 'p_dropout': 0.3, 'nonlin_func': 'nn.ReLU()', 'mlp_embed_factor': 2, 'pooling_mode': 'attn', 'dist_opt': 'cosine', 'base_lr': 0.0003, 'max_lr_mul': 10, 'l2_reg': 0.001, 'loss_w': 1.0, 'margin_v': 1.0, 'expression_dim': 64, 'expression_input_size': 908, 'exp_H1': 4096, 'exp_H2': 1024}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/bedict_crispr/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout(p=0.3, inplace=False) Dropout(p=0.3, inplace=False)\n",
      "=====Epoch 0\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   3%|█                               | 9/271 [00:02<01:09,  3.79it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m     create_directory(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(exp_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     14\u001b[0m     create_directory(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(exp_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelstates\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mdeepadr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_functions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_exp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnd: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/mnt/graphnn/deepadr/train_functions.py:174\u001b[0m, in \u001b[0;36mrun_exp\u001b[0;34m(queue, used_dataset, gpu_num, tp, exp_dir, partition)\u001b[0m\n\u001b[1;32m    170\u001b[0m logsoftmax_scores \u001b[38;5;241m=\u001b[39m expression_model(triplet)\n\u001b[1;32m    172\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_nlll(logsoftmax_scores, batch\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mlong))            \n\u001b[0;32m--> 174\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Derive gradients.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update parameters based on gradients.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m cyc_scheduler\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# after each batch step the scheduler\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bedict_crispr/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/bedict_crispr/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#q_i = 0\n",
    "#partition = fold_partitions[q_i]\n",
    "partition = fold_partitions\n",
    "gpu_num = 1\n",
    "time_stamp = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "import torch.multiprocessing as mp\n",
    "mp.set_start_method(\"spawn\", force=True)\n",
    "queue = mp.Queue()\n",
    "\n",
    "for i in range(len(fold_partitions)):\n",
    "    partition = fold_partitions[i]\n",
    "    exp_dir = create_directory(os.path.join(targetdata_dir_exp, str(partition_style)+\"_fold_\"+str(i)+\"_\"+time_stamp))\n",
    "    create_directory(os.path.join(exp_dir, \"predictions\"))\n",
    "    create_directory(os.path.join(exp_dir, \"modelstates\"))\n",
    "    deepadr.train_functions.run_exp(queue, dataset, 0, tp, exp_dir, partition)\n",
    "print(\"End: \" + datetime.datetime.now().strftime('%Y-%m-%d')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deepadr.train_functions.run_exp(queue, used_dataset, gpu_num, tp, exp_dir, partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### muti-processing\n",
    "time_stamp = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "import torch.multiprocessing as mp\n",
    "mp.set_start_method(\"spawn\", force=True)\n",
    "queue = mp.Queue()\n",
    "q_processes = []\n",
    "def spawn_q_process(q_process):\n",
    "    print(\">>> spawning hyperparam search process\")\n",
    "    q_process.start()\n",
    "    \n",
    "def join_q_process(q_process):\n",
    "    q_process.join()\n",
    "    print(\"<<< joined hyperparam search process\")\n",
    "    \n",
    "def create_q_process(queue, dataset, gpu_num, tphp, exp_dir, partition):\n",
    "#     fold_gpu_map = {0:gpu_num}\n",
    "    return mp.Process(target=deepadr.train_functions.run_exp, args=(queue, dataset, gpu_num, tphp, exp_dir, partition))\n",
    "\n",
    "print(\"Start: \" + time_stamp)\n",
    "for q_i in range(min(n_gpu, len(fold_partitions))):\n",
    "    partition = fold_partitions[q_i]\n",
    "    print(q_i)\n",
    "    exp_dir = create_directory(os.path.join(targetdata_dir_exp, \"fold_\"+str(q_i)+\"_\"+time_stamp))\n",
    "    create_directory(os.path.join(exp_dir, \"predictions\"))\n",
    "    create_directory(os.path.join(exp_dir, \"modelstates\"))\n",
    "    q_process = create_q_process(queue, dataset, q_i, tp, exp_dir, partition)\n",
    "    q_processes.append(q_process)\n",
    "    spawn_q_process(q_process)\n",
    "\n",
    "spawned_processes = n_gpu\n",
    "    \n",
    "for q_i in range(min(n_gpu, len(fold_partitions))):\n",
    "    join_q_process(q_processes[q_i])\n",
    "    released_gpu_num = queue.get()\n",
    "    print(\"released_gpu_num:\", released_gpu_num)\n",
    "    \n",
    "print(\"End: \" + datetime.datetime.now().strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
