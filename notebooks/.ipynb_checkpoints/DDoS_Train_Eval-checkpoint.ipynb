{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__='2.0.0+cu117'\n",
      "torch.__file__='/home/ubuntu/anaconda3/envs/bedict_crispr/lib/python3.10/site-packages/torch/__init__.py'\n",
      "torch.cuda.device_count()=1\n",
      "torch.cuda.is_available()=True\n",
      "torch.version.cuda='11.7'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/bedict_crispr/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.executable\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import ogb\n",
    "from tqdm import tqdm\n",
    "import hiplot as hip\n",
    "from copy import deepcopy\n",
    "import datetime\n",
    "import json\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import Subset, TensorDataset\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "print(f\"{torch.__version__=}\")\n",
    "print(f\"{torch.__file__=}\")\n",
    "print(f\"{torch.cuda.device_count()=}\")\n",
    "print(f\"{torch.cuda.is_available()=}\")\n",
    "print(f\"{torch.version.cuda=}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mnt/graphnn/notebooks\n",
      "/home/ubuntu/mnt/graphnn\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "cwd_parent = os.path.abspath(os.path.join(cwd, os.pardir))\n",
    "print(cwd_parent)\n",
    "\n",
    "sys.path.append(cwd_parent)\n",
    "\n",
    "import deepadr\n",
    "from deepadr.dataset import *\n",
    "from deepadr.utilities import *\n",
    "from deepadr.chemfeatures import *\n",
    "from deepadr.train_functions import *\n",
    "from deepadr.model_gnn_ogb import GNN, DeepAdr_SiameseTrf, ExpressionNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version\n",
      "3.10.9 (main, Jan 11 2023, 15:21:40) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python version\")\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_dir = '../data/raw/'\n",
    "processed_dir = '../data/processed/'\n",
    "up_dir = '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of GPUs available: 1\n",
      "cuda:0, name:Tesla T4\n",
      "total memory available: 14.755615234375 GB\n",
      "total memory allocated on device: 0.0 GB\n",
      "max memory allocated on device: 0.0 GB\n",
      "total memory cached on device: 0.0 GB\n",
      "max memory cached  on device: 0.0 GB\n",
      "\n",
      "torch: 2.0.0+cu117\n",
      "CUDA: 11.7\n",
      "3.10.9 (main, Jan 11 2023, 15:21:40) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "report_available_cuda_devices()\n",
    "n_gpu = torch.cuda.device_count()\n",
    "n_gpu\n",
    "device_cpu = get_device(to_gpu=True)\n",
    "# device_gpu = get_device(True, index=0)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"CUDA:\", torch.version.cuda)\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options: \n",
    "# 'total_thresh' + 4,3,2\n",
    "# 'loewe_thresh', 'hsa_thresh', 'bliss_thresh', 'zip_thresh' + 1\n",
    "\n",
    "score = 'loewe_thresh'\n",
    "score_val = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DrugComb_loewe_thresh_1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DSdataset_name = f'DrugComb_{score}_{score_val}'\n",
    "data_fname = 'data_v1' # v2 for baseline models, v3 for additive samples\n",
    "DSdataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mnt/graphnn/data/processed/DrugComb_loewe_thresh_1/data_v1\n"
     ]
    }
   ],
   "source": [
    "targetdata_dir = create_directory(os.path.join(processed_dir, DSdataset_name, data_fname))\n",
    "targetdata_dir_raw = create_directory(os.path.join(targetdata_dir, \"raw\"))\n",
    "targetdata_dir_processed = create_directory(os.path.join(targetdata_dir, \"processed\"))\n",
    "targetdata_dir_exp = create_directory(os.path.join(targetdata_dir, \"experiments\"))\n",
    "# # ReaderWriter.dump_data(dpartitions, os.path.join(targetdata_dir, 'data_partitions.pkl'))\n",
    "print(targetdata_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.51 ms, sys: 437 ms, total: 446 ms\n",
      "Wall time: 445 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Make sure to first run the \"DDoS_Dataset_Generation\" notebook first\n",
    "dataset = MoleculeDataset(root=targetdata_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(targetdata_dir_raw +'/data_pairs.pkl', 'rb') as file:\n",
    "    data_pairs = pickle.load(file )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drug1_ID</th>\n",
       "      <th>Drug2_ID</th>\n",
       "      <th>Cell_Line_ID</th>\n",
       "      <th>Cosmic_ID</th>\n",
       "      <th>Drug1</th>\n",
       "      <th>Drug2</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lonidamine</td>\n",
       "      <td>717906-29-1</td>\n",
       "      <td>A-673</td>\n",
       "      <td>684052</td>\n",
       "      <td>C1=CC=C2C(=C1)C(=NN2CC3=C(C=C(C=C3)Cl)Cl)C(=O)O</td>\n",
       "      <td>CN(C1=CC=CC=C1CNC2=NC(=NC=C2C(F)(F)F)NC3=CC4=C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ethyl bromopyruvate</td>\n",
       "      <td>717906-29-1</td>\n",
       "      <td>A-673</td>\n",
       "      <td>684052</td>\n",
       "      <td>CCOC(=O)C(=O)CBr</td>\n",
       "      <td>CN(C1=CC=CC=C1CNC2=NC(=NC=C2C(F)(F)F)NC3=CC4=C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tranilast (trans-)</td>\n",
       "      <td>717906-29-1</td>\n",
       "      <td>A-673</td>\n",
       "      <td>684052</td>\n",
       "      <td>COC1=C(C=C(C=C1)C=CC(=O)NC2=CC=CC=C2C(=O)O)OC</td>\n",
       "      <td>CN(C1=CC=CC=C1CNC2=NC(=NC=C2C(F)(F)F)NC3=CC4=C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lenalidomide</td>\n",
       "      <td>717906-29-1</td>\n",
       "      <td>A-673</td>\n",
       "      <td>684052</td>\n",
       "      <td>C1CC(=O)NC(=O)C1N2CC3=C(C2=O)C=CC=C3N</td>\n",
       "      <td>CN(C1=CC=CC=C1CNC2=NC(=NC=C2C(F)(F)F)NC3=CC4=C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thalidomide</td>\n",
       "      <td>717906-29-1</td>\n",
       "      <td>A-673</td>\n",
       "      <td>684052</td>\n",
       "      <td>C1CC(=O)NC(=O)C1N2C(=O)C3=CC=CC=C3C2=O</td>\n",
       "      <td>CN(C1=CC=CC=C1CNC2=NC(=NC=C2C(F)(F)F)NC3=CC4=C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163811</th>\n",
       "      <td>AZD1208</td>\n",
       "      <td>AZD6738</td>\n",
       "      <td>VM-CUB-1</td>\n",
       "      <td>909780</td>\n",
       "      <td>C1CC(CN(C1)C2=C(C=CC=C2C=C3C(=O)NC(=O)S3)C4=CC...</td>\n",
       "      <td>CC1COCCN1C2=NC(=NC(=C2)C3(CC3)S(=N)(=O)C)C4=C5...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163812</th>\n",
       "      <td>AZD1208</td>\n",
       "      <td>Vorinostat</td>\n",
       "      <td>VM-CUB-1</td>\n",
       "      <td>909780</td>\n",
       "      <td>C1CC(CN(C1)C2=C(C=CC=C2C=C3C(=O)NC(=O)S3)C4=CC...</td>\n",
       "      <td>C1=CC=C(C=C1)NC(=O)CCCCCCC(=O)NO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163813</th>\n",
       "      <td>AZD1208</td>\n",
       "      <td>Onalespib</td>\n",
       "      <td>VM-CUB-1</td>\n",
       "      <td>909780</td>\n",
       "      <td>C1CC(CN(C1)C2=C(C=CC=C2C=C3C(=O)NC(=O)S3)C4=CC...</td>\n",
       "      <td>CC(C)C1=C(C=C(C(=C1)C(=O)N2CC3=C(C2)C=C(C=C3)C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163814</th>\n",
       "      <td>Saracatinib</td>\n",
       "      <td>Onalespib</td>\n",
       "      <td>VM-CUB-1</td>\n",
       "      <td>909780</td>\n",
       "      <td>CN1CCN(CC1)CCOC2=CC(=C3C(=C2)N=CN=C3NC4=C(C=CC...</td>\n",
       "      <td>CC(C)C1=C(C=C(C(=C1)C(=O)N2CC3=C(C2)C=C(C=C3)C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163815</th>\n",
       "      <td>Saracatinib</td>\n",
       "      <td>AZD1208</td>\n",
       "      <td>VM-CUB-1</td>\n",
       "      <td>909780</td>\n",
       "      <td>CN1CCN(CC1)CCOC2=CC(=C3C(=C2)N=CN=C3NC4=C(C=CC...</td>\n",
       "      <td>C1CC(CN(C1)C2=C(C=CC=C2C=C3C(=O)NC(=O)S3)C4=CC...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>163816 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Drug1_ID     Drug2_ID Cell_Line_ID  Cosmic_ID   \n",
       "0                lonidamine  717906-29-1        A-673     684052  \\\n",
       "1       Ethyl bromopyruvate  717906-29-1        A-673     684052   \n",
       "2        Tranilast (trans-)  717906-29-1        A-673     684052   \n",
       "3              Lenalidomide  717906-29-1        A-673     684052   \n",
       "4               thalidomide  717906-29-1        A-673     684052   \n",
       "...                     ...          ...          ...        ...   \n",
       "163811              AZD1208      AZD6738     VM-CUB-1     909780   \n",
       "163812              AZD1208   Vorinostat     VM-CUB-1     909780   \n",
       "163813              AZD1208    Onalespib     VM-CUB-1     909780   \n",
       "163814          Saracatinib    Onalespib     VM-CUB-1     909780   \n",
       "163815          Saracatinib      AZD1208     VM-CUB-1     909780   \n",
       "\n",
       "                                                    Drug1   \n",
       "0         C1=CC=C2C(=C1)C(=NN2CC3=C(C=C(C=C3)Cl)Cl)C(=O)O  \\\n",
       "1                                        CCOC(=O)C(=O)CBr   \n",
       "2           COC1=C(C=C(C=C1)C=CC(=O)NC2=CC=CC=C2C(=O)O)OC   \n",
       "3                   C1CC(=O)NC(=O)C1N2CC3=C(C2=O)C=CC=C3N   \n",
       "4                  C1CC(=O)NC(=O)C1N2C(=O)C3=CC=CC=C3C2=O   \n",
       "...                                                   ...   \n",
       "163811  C1CC(CN(C1)C2=C(C=CC=C2C=C3C(=O)NC(=O)S3)C4=CC...   \n",
       "163812  C1CC(CN(C1)C2=C(C=CC=C2C=C3C(=O)NC(=O)S3)C4=CC...   \n",
       "163813  C1CC(CN(C1)C2=C(C=CC=C2C=C3C(=O)NC(=O)S3)C4=CC...   \n",
       "163814  CN1CCN(CC1)CCOC2=CC(=C3C(=C2)N=CN=C3NC4=C(C=CC...   \n",
       "163815  CN1CCN(CC1)CCOC2=CC(=C3C(=C2)N=CN=C3NC4=C(C=CC...   \n",
       "\n",
       "                                                    Drug2  Y  \n",
       "0       CN(C1=CC=CC=C1CNC2=NC(=NC=C2C(F)(F)F)NC3=CC4=C...  0  \n",
       "1       CN(C1=CC=CC=C1CNC2=NC(=NC=C2C(F)(F)F)NC3=CC4=C...  0  \n",
       "2       CN(C1=CC=CC=C1CNC2=NC(=NC=C2C(F)(F)F)NC3=CC4=C...  0  \n",
       "3       CN(C1=CC=CC=C1CNC2=NC(=NC=C2C(F)(F)F)NC3=CC4=C...  0  \n",
       "4       CN(C1=CC=CC=C1CNC2=NC(=NC=C2C(F)(F)F)NC3=CC4=C...  0  \n",
       "...                                                   ... ..  \n",
       "163811  CC1COCCN1C2=NC(=NC(=C2)C3(CC3)S(=N)(=O)C)C4=C5...  0  \n",
       "163812                   C1=CC=C(C=C1)NC(=O)CCCCCCC(=O)NO  0  \n",
       "163813  CC(C)C1=C(C=C(C(=C1)C(=O)N2CC3=C(C2)C=C(C=C3)C...  0  \n",
       "163814  CC(C)C1=C(C=C(C(=C1)C(=O)N2CC3=C(C2)C=C(C=C3)C...  0  \n",
       "163815  C1CC(CN(C1)C2=C(C=CC=C2C=C3C(=O)NC(=O)S3)C4=CC...  0  \n",
       "\n",
       "[163816 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data_pairs.groupby('Cell_Line_ID')['Y'].value_counts(normalize=True).unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         22Rv1\n",
       "1         451Lu\n",
       "2          647V\n",
       "3         786-0\n",
       "4         A-673\n",
       "         ...   \n",
       "159       UO-31\n",
       "160     UWB1289\n",
       "161        VCAP\n",
       "162    VM-CUB-1\n",
       "163       WM115\n",
       "Name: Cell_Line_ID, Length: 164, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = grouped.reset_index()\n",
    "grouped['Cell_Line_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This file is to leav a cell out partition\n",
    "def data_partition_leave_cells_out(num_folds,random_state,valid_set_portion,path_to_save, feature ):\n",
    "    import pickle\n",
    "    with open(targetdata_dir_raw +'/data_pairs.pkl', 'rb') as file:\n",
    "        data_pairs = pickle.load(file )\n",
    "        \n",
    "    #grouped = data_pairs.groupby('Cosmic_ID')['Y'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "    grouped = data_pairs.groupby(feature)['Y'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "    ### we lable those have mostly 0 as class0, blanced 0 and 1 as class 1 and similar distribution to \n",
    "    ## the original class distribution, around 0.81 as 2 and more balanced ones are 3\n",
    "    label = []\n",
    "    for i in range(len(grouped)):\n",
    "        if grouped[0].iloc[i]>=0.95:\n",
    "            label.append(0)\n",
    "        elif grouped[0].iloc[i]<0.2:\n",
    "            label.append(1)\n",
    "        elif grouped[0].iloc[i]<0.95 and grouped[0].iloc[i]>0.75:\n",
    "            label.append(2)\n",
    "        else:\n",
    "            label.append(3)   \n",
    "            \n",
    "    grouped['label'] = label\n",
    "    grouped = grouped.reset_index()\n",
    "    \n",
    "    X = grouped[feature]\n",
    "    Y = grouped['label']\n",
    "    \n",
    "    skf_trte = StratifiedKFold(n_splits=num_folds, random_state=random_state, shuffle=True)  # split train and test\n",
    "    skf_trv = StratifiedShuffleSplit(n_splits=2, \n",
    "                                     test_size=valid_set_portion, \n",
    "                                     random_state=random_state)  # split train and validation\n",
    "\n",
    "    data_partition = {}\n",
    "    fold_num = 0\n",
    "    \n",
    "    for train_index, test_index in skf_trte.split(X,Y):\n",
    "        x_tr = np.zeros(len(train_index))\n",
    "        y_tr = Y[train_index] \n",
    "        #print('train cell line number', len(train_index))\n",
    "        print('test', feature,  'number', len(test_index))\n",
    "\n",
    "        for tr_index, val_index in skf_trv.split(x_tr, y_tr):\n",
    "            tr_ids = train_index[tr_index]\n",
    "            val_ids = train_index[val_index]\n",
    "            print('train', feature ,' number', len(tr_ids))\n",
    "            print('validation' feature,  'number' , len(val_ids))\n",
    "            \n",
    "            data_partition[fold_num] = {'train': tr_ids,\n",
    "                                             'validation': val_ids,\n",
    "                                             'test': test_index}\n",
    "            \n",
    "            \n",
    "            \n",
    "        fold_num = fold_num + 1\n",
    "    \n",
    "    \n",
    "    data_partitions = {}\n",
    "   \n",
    "    YY = data_pairs.Y\n",
    "    for i in range(num_folds):\n",
    "        train_cell = X[data_partition[i]['train']]\n",
    "        val_cell = X[data_partition[i]['validation']]\n",
    "        test_cell = X[data_partition[i]['test']]\n",
    "        train_indices = np.array(data_pairs.loc[data_pairs[feature].isin(train_cell)].index)\n",
    "        val_indices = np.array(data_pairs.loc[data_pairs[feature].isin(val_cell)].index)\n",
    "        test_indices = np.array(data_pairs.loc[data_pairs[feature].isin(test_cell)].index)\n",
    "        data_partitions[i] = {'train': train_indices,\n",
    "                                             'validation': val_indices,\n",
    "                                             'test': test_indices}\n",
    "        \n",
    "    \n",
    "    \n",
    "        print(\"fold_num:\", i)\n",
    "        print('train data')\n",
    "        report_label_distrib(YY[data_partitions[i]['train']])\n",
    "        print('validation data')\n",
    "        report_label_distrib(YY[data_partitions[i]['validation']])\n",
    "        print('test data')\n",
    "        report_label_distrib(YY[data_partitions[i]['test']])\n",
    "        print()\n",
    "        fold_num += 1\n",
    "        print(\"-\"*25)\n",
    "    \n",
    "    file_name = path_to_save + f'/partions_{feature}_out.pkl'\n",
    "    with open(file_name, \"wb\") as pickle_file:\n",
    "        pickle.dump(data_partitions, pickle_file)\n",
    "    return data_partitions\n",
    "\n",
    "\n",
    "def report_label_distrib(labels):\n",
    "    classes, counts = np.unique(labels, return_counts=True)\n",
    "    norm_counts = counts/counts.sum()\n",
    "    for i, label in enumerate(classes):\n",
    "        print(\"class:\", label, \"norm count:\", norm_counts[i])\n",
    "\n",
    "\n",
    "def validate_partitioan(train_indices, val_indices, test_indices):\n",
    "    # Find the intersection between test_indices and val_indices\n",
    "    test_val_intersection = set(test_indices).intersection(val_indices)\n",
    "\n",
    "    # Find the intersection between test_indices and train_indices\n",
    "    test_train_intersection = set(test_indices).intersection(train_indices)\n",
    "\n",
    "    # Find the intersection between val_indices and train_indices\n",
    "    val_train_intersection = set(val_indices).intersection(train_indices)\n",
    "\n",
    "    # Check if any of the intersections have common elements\n",
    "    if test_val_intersection or test_train_intersection or val_train_intersection:\n",
    "        print(\"The sets of indices intersect.\")\n",
    "    else:\n",
    "        print(\"The sets of indices do not intersect.\")\n",
    "\n",
    "    # Calculate the total count of indices\n",
    "    total_indices_count = len(test_indices) + len(val_indices) + len(train_indices)\n",
    "\n",
    "    # Print the total count of indices\n",
    "    print(\"Total count of indices:\", total_indices_count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test cell line number 33\n",
      "train cell line number 117\n",
      "validation cell line number 14\n",
      "train cell line number 117\n",
      "validation cell line number 14\n",
      "test cell line number 33\n",
      "train cell line number 117\n",
      "validation cell line number 14\n",
      "train cell line number 117\n",
      "validation cell line number 14\n",
      "test cell line number 33\n",
      "train cell line number 117\n",
      "validation cell line number 14\n",
      "train cell line number 117\n",
      "validation cell line number 14\n",
      "test cell line number 33\n",
      "train cell line number 117\n",
      "validation cell line number 14\n",
      "train cell line number 117\n",
      "validation cell line number 14\n",
      "test cell line number 32\n",
      "train cell line number 118\n",
      "validation cell line number 14\n",
      "train cell line number 118\n",
      "validation cell line number 14\n",
      "fold_num: 0\n",
      "train data\n",
      "class: 0 norm count: 0.8457189975857731\n",
      "class: 1 norm count: 0.15428100241422693\n",
      "validation data\n",
      "class: 0 norm count: 0.8643281927225078\n",
      "class: 1 norm count: 0.1356718072774922\n",
      "test data\n",
      "class: 0 norm count: 0.8695220496980061\n",
      "class: 1 norm count: 0.130477950301994\n",
      "\n",
      "-------------------------\n",
      "fold_num: 1\n",
      "train data\n",
      "class: 0 norm count: 0.8683082805907173\n",
      "class: 1 norm count: 0.1316917194092827\n",
      "validation data\n",
      "class: 0 norm count: 0.8017420157610949\n",
      "class: 1 norm count: 0.198257984238905\n",
      "test data\n",
      "class: 0 norm count: 0.8079697207741199\n",
      "class: 1 norm count: 0.19203027922588017\n",
      "\n",
      "-------------------------\n",
      "fold_num: 2\n",
      "train data\n",
      "class: 0 norm count: 0.8483830734240366\n",
      "class: 1 norm count: 0.15161692657596335\n",
      "validation data\n",
      "class: 0 norm count: 0.8112057448229792\n",
      "class: 1 norm count: 0.1887942551770207\n",
      "test data\n",
      "class: 0 norm count: 0.8846035174328911\n",
      "class: 1 norm count: 0.11539648256710892\n",
      "\n",
      "-------------------------\n",
      "fold_num: 3\n",
      "train data\n",
      "class: 0 norm count: 0.8526704635621463\n",
      "class: 1 norm count: 0.14732953643785376\n",
      "validation data\n",
      "class: 0 norm count: 0.8454347650898542\n",
      "class: 1 norm count: 0.15456523491014587\n",
      "test data\n",
      "class: 0 norm count: 0.8526368447381586\n",
      "class: 1 norm count: 0.14736315526184146\n",
      "\n",
      "-------------------------\n",
      "fold_num: 4\n",
      "train data\n",
      "class: 0 norm count: 0.8545911178157585\n",
      "class: 1 norm count: 0.1454088821842415\n",
      "validation data\n",
      "class: 0 norm count: 0.8580183861082737\n",
      "class: 1 norm count: 0.14198161389172625\n",
      "test data\n",
      "class: 0 norm count: 0.8409308354680411\n",
      "class: 1 norm count: 0.15906916453195888\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "#data_partitions =data_partition_leave_cells_out(5, 42, 0.1, targetdata_dir_processed,'Drug1_ID' )\n",
    "partition_style = 'Cell_Line_ID'\n",
    "data_partitions =data_partition_leave_cells_out(5, 42, 0.1, targetdata_dir_processed,partition_style )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used_dataset = dataset\n",
    "# If you want to use a smaller subset of the dataset for testing\n",
    "#smaller_dataset_len = int(len(dataset)/100)\n",
    "#used_dataset = dataset[:smaller_dataset_len]\n",
    "\n",
    "#dataset = used_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "fold_partitions = get_stratified_partitions(dataset.data.y,\n",
    "                                            num_folds=5,\n",
    "                                            valid_set_portion=0.1,\n",
    "                                            random_state=42)\n",
    "                                            \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training graphs: \"+ str(len(fold_partitions[0]['train'])))\n",
    "print(\"Number of validation graphs: \"+ str(len(fold_partitions[0]['validation'])))\n",
    "print(\"Number of testing graphs: \"+ str(len(fold_partitions[0]['test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "\n",
    "tp = {\n",
    "    \"batch_size\" : 300,\n",
    "    \"num_epochs\" : 100,\n",
    "    \n",
    "    \"emb_dim\" : 100,\n",
    "    \"gnn_type\" : \"gatv2\",\n",
    "    \"num_layer\" : 5,\n",
    "    \"graph_pooling\" : \"mean\", #attention\n",
    "    \n",
    "    \"input_embed_dim\" : None,\n",
    "    \"gene_embed_dim\": 1,\n",
    "    \"num_attn_heads\" : 2,\n",
    "    \"num_transformer_units\" : 1,\n",
    "    \"p_dropout\" : 0.3,\n",
    "    \"nonlin_func\" : 'nn.ReLU()',\n",
    "    \"mlp_embed_factor\" : 2,\n",
    "    \"pooling_mode\" : 'attn',\n",
    "    \"dist_opt\" : 'cosine',\n",
    "\n",
    "    \"base_lr\" : 3e-4, #3e-4\n",
    "    \"max_lr_mul\": 10,\n",
    "    \"l2_reg\" : 1e-7,\n",
    "    \"loss_w\" : 1.,\n",
    "    \"margin_v\" : 1.,\n",
    "\n",
    "    \"expression_dim\" : 64,\n",
    "    \"expression_input_size\" : 908,\n",
    "    \"exp_H1\" : 4096,\n",
    "    \"exp_H2\" : 1024\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#q_i = 0\n",
    "#partition = fold_partitions[q_i]\n",
    "partition = fold_partitions\n",
    "gpu_num = 1\n",
    "time_stamp = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "import torch.multiprocessing as mp\n",
    "mp.set_start_method(\"spawn\", force=True)\n",
    "queue = mp.Queue()\n",
    "\n",
    "for i in range(len(fold_partitions)):\n",
    "    partition = fold_partitions[i]\n",
    "    exp_dir = create_directory(os.path.join(targetdata_dir_exp, str(partition_style)+\"_fold_\"+str(i)+\"_\"+time_stamp))\n",
    "    create_directory(os.path.join(exp_dir, \"predictions\"))\n",
    "    create_directory(os.path.join(exp_dir, \"modelstates\"))\n",
    "    deepadr.train_functions.run_exp(queue, dataset, 0, tp, exp_dir, partition)\n",
    "print(\"End: \" + datetime.datetime.now().strftime('%Y-%m-%d')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deepadr.train_functions.run_exp(queue, used_dataset, gpu_num, tp, exp_dir, partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### muti-processing\n",
    "time_stamp = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "import torch.multiprocessing as mp\n",
    "mp.set_start_method(\"spawn\", force=True)\n",
    "queue = mp.Queue()\n",
    "q_processes = []\n",
    "def spawn_q_process(q_process):\n",
    "    print(\">>> spawning hyperparam search process\")\n",
    "    q_process.start()\n",
    "    \n",
    "def join_q_process(q_process):\n",
    "    q_process.join()\n",
    "    print(\"<<< joined hyperparam search process\")\n",
    "    \n",
    "def create_q_process(queue, dataset, gpu_num, tphp, exp_dir, partition):\n",
    "#     fold_gpu_map = {0:gpu_num}\n",
    "    return mp.Process(target=deepadr.train_functions.run_exp, args=(queue, dataset, gpu_num, tphp, exp_dir, partition))\n",
    "\n",
    "print(\"Start: \" + time_stamp)\n",
    "for q_i in range(min(n_gpu, len(fold_partitions))):\n",
    "    partition = fold_partitions[q_i]\n",
    "    print(q_i)\n",
    "    exp_dir = create_directory(os.path.join(targetdata_dir_exp, \"fold_\"+str(q_i)+\"_\"+time_stamp))\n",
    "    create_directory(os.path.join(exp_dir, \"predictions\"))\n",
    "    create_directory(os.path.join(exp_dir, \"modelstates\"))\n",
    "    q_process = create_q_process(queue, dataset, q_i, tp, exp_dir, partition)\n",
    "    q_processes.append(q_process)\n",
    "    spawn_q_process(q_process)\n",
    "\n",
    "spawned_processes = n_gpu\n",
    "    \n",
    "for q_i in range(min(n_gpu, len(fold_partitions))):\n",
    "    join_q_process(q_processes[q_i])\n",
    "    released_gpu_num = queue.get()\n",
    "    print(\"released_gpu_num:\", released_gpu_num)\n",
    "    \n",
    "print(\"End: \" + datetime.datetime.now().strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
